# LLaMA Factory image for NVIDIA DGX-SPARK (ARM64 / Grace Blackwell).
# Build on DGX-SPARK or any linux/arm64 host:
#   docker build -f docker/docker-cuda/Dockerfile.dgx-spark -t llamafactory:dgx-spark .
# See: https://build.nvidia.com/spark/llama-factory

# NGC PyTorch container for Grace Blackwell (ARM64); follow DGX Spark playbook
# https://build.nvidia.com/spark/llama-factory / https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch
ARG BASE_IMAGE=nvcr.io/nvidia/pytorch:25.11-py3
FROM ${BASE_IMAGE}

# Installation arguments
ARG PIP_INDEX=https://pypi.org/simple
ARG HTTP_PROXY=""

ENV DEBIAN_FRONTEND=noninteractive
ENV PIP_ROOT_USER_ACTION=ignore
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn
ENV NODE_OPTIONS=""
ENV http_proxy="${HTTP_PROXY}"
ENV https_proxy="${HTTP_PROXY}"

SHELL ["/bin/bash", "-c"]
WORKDIR /app

# Use optional pip mirror
RUN pip config set global.index-url "${PIP_INDEX}" && \
    pip config set global.extra-index-url "${PIP_INDEX}" && \
    pip install --no-cache-dir --upgrade pip packaging wheel setuptools editables "hatchling>=1.18.0"

# Copy repo (build context = repository root)
COPY . /app

# Install LLaMA Factory and metrics/deepspeed; rely on NGC container's preinstalled torch/torchaudio.
RUN pip install --no-cache-dir --no-build-isolation -e . && \
    pip install --no-cache-dir --no-build-isolation -r requirements/metrics.txt -r requirements/deepspeed.txt || true

# Expose ports
ENV GRADIO_SERVER_PORT=7860
ENV API_PORT=8000
EXPOSE 7860 8000

# Unset proxy
ENV http_proxy=
ENV https_proxy=

RUN pip config unset global.index-url 2>/dev/null || true && \
    pip config unset global.extra-index-url 2>/dev/null || true

# Default: interactive shell (same as docker-compose for other images)
CMD ["bash"]
